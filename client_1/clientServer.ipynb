{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77416f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 10:31:11.492948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/slurm-20.11.0/lib64:\n",
      "2022-05-26 10:31:11.492998: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import contextlib\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import spur\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36209fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the data and create train/test split\n",
    "def dataloader(path):\n",
    "    ##load features and cohort data\n",
    "    X = pd.read_csv(path+'X.csv', index_col = 'hadm_id')\n",
    "    y = pd.read_csv(path+'y.csv', index_col = 'hadm_id')\n",
    "\n",
    "    ## train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, random_state=1)\n",
    "    #create validation set too\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "    ## create scaler and apply only to numeric data before adding binary data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train.iloc[:,:-5])\n",
    "    X_train_norm = pd.DataFrame(X_train_norm, index = X_train.index, columns = X_train.columns[:-5])\n",
    "    X_train_norm = X_train_norm.merge(X_train.iloc[:,-5:], left_index = True, right_index = True)\n",
    "\n",
    "    ##apply scaler to test data\n",
    "    X_test_norm = scaler.transform(X_test.iloc[:,:-5])\n",
    "    X_test_norm = pd.DataFrame(X_test_norm, index = X_test.index, columns = X_test.columns[:-5])\n",
    "    X_test_norm = X_test_norm.merge(X_test.iloc[:,-5:], left_index = True, right_index = True)\n",
    "    \n",
    "    ##apply scaler to val data\n",
    "    X_val_norm = scaler.transform(X_val.iloc[:,:-5])\n",
    "    X_val_norm = pd.DataFrame(X_val_norm, index = X_val.index, columns = X_val.columns[:-5])\n",
    "    X_val_norm = X_val_norm.merge(X_val.iloc[:,-5:], left_index = True, right_index = True)\n",
    "    \n",
    "    return X_train_norm, X_test_norm, X_val_norm, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d68567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create the keras model (LR in this case)\n",
    "def create_keras_model():\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
    "    ##build LR model\n",
    "    number_of_classes = 1\n",
    "    number_of_features = X_train_norm.shape[1]\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes,activation = 'sigmoid',input_dim = number_of_features))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc610da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read in the arguments provided by the master server\n",
    "    client, usr0, pwd0, centralServer, epochs = [sys.argv[i] for i in range(1, 6)]\n",
    "    __file__ = 'clientServer.ipynb'\n",
    "    dir = os.path.abspath(os.path.dirname(__file__)) \n",
    "    path = '/gpfs/commons/groups/gursoy_lab/aelhussein/DCI_FL/'\n",
    "    epochs = int(epochs)\n",
    "\n",
    "    # Import the data\n",
    "    X_train_norm, X_test_norm, X_val_norm, y_train, y_test, y_val = dataloader(dir+'/Data/')\n",
    "\n",
    "    # Load and run the neural network\n",
    "    client_model = tf.keras.models.load_model(f'{dir}/model/current_model')\n",
    "\n",
    "    # Evaluate on the validation set    \n",
    "    validate_loss, validate_auc = client_model.evaluate(X_val_norm, y_val, verbose=0)\n",
    "\n",
    "    # Train model\n",
    "    history = tf.keras.callbacks.History()\n",
    "    with contextlib.redirect_stdout(None):\n",
    "        client_model.fit(X_train_norm, y_train, verbose=0, epochs=epochs, callbacks=[history])\n",
    "\n",
    "    # Save model\n",
    "    client_model.save(f'{dir}/model')\n",
    "\n",
    "    # Calculate loss + auc\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    train_auc = history.history['auc'][-1]\n",
    "\n",
    "    # Send the weights back to master server\n",
    "    shell = spur.LocalShell()\n",
    "    command_1 = f'sshpass -p {pwd0} scp -r {dir}/model {usr0}@{centralServer}:{path}server/model/client_models/{client}'\n",
    "    command_1 = command_1.split(' ')\n",
    "    shell.run(command_1)\n",
    "\n",
    "    # Reset stdout and print\n",
    "    print(len(X_train_norm), train_loss, train_auc, validate_loss, validate_auc)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
