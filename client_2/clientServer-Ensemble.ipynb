{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "77416f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import contextlib\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import spur\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36209fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the data and create train/test split\n",
    "def dataloader(path):\n",
    "    ##load features and cohort data\n",
    "    X = pd.read_csv(path+'X.csv', index_col = 'hadm_id')\n",
    "    y = pd.read_csv(path+'y.csv', index_col = 'hadm_id')\n",
    "\n",
    "    ## train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, random_state=1)\n",
    "    #create validation set too\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "    ## create scaler and apply only to numeric data before adding binary data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train.iloc[:,:-5])\n",
    "    X_train_norm = pd.DataFrame(X_train_norm, index = X_train.index, columns = X_train.columns[:-5])\n",
    "    X_train_norm = X_train_norm.merge(X_train.iloc[:,-5:], left_index = True, right_index = True)\n",
    "\n",
    "    ##apply scaler to test data\n",
    "    X_test_norm = scaler.transform(X_test.iloc[:,:-5])\n",
    "    X_test_norm = pd.DataFrame(X_test_norm, index = X_test.index, columns = X_test.columns[:-5])\n",
    "    X_test_norm = X_test_norm.merge(X_test.iloc[:,-5:], left_index = True, right_index = True)\n",
    "    \n",
    "    ##apply scaler to val data\n",
    "    X_val_norm = scaler.transform(X_val.iloc[:,:-5])\n",
    "    X_val_norm = pd.DataFrame(X_val_norm, index = X_val.index, columns = X_val.columns[:-5])\n",
    "    X_val_norm = X_val_norm.merge(X_val.iloc[:,-5:], left_index = True, right_index = True)\n",
    "    \n",
    "    return X_train_norm, X_test_norm, X_val_norm, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d68567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_probability_clients(path, client):\n",
    "    models = ['LR', 'RF', 'SVM']\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        command = f'python {path}{client}/clientServer-{model}.py -cl={client} -ts=Pass -en=True'\n",
    "        command = command.split(' ')\n",
    "        output = subprocess.check_output(command) \n",
    "        output =  output.decode('utf-8').split('\\n')\n",
    "        results[model]= prob_parser(output)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eb27142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_parser(output):\n",
    "    parsed = []\n",
    "    for i in output:\n",
    "        parsed.extend([float(x) for x in i.replace('[', '').replace(']','').split(' ') if x != ''])\n",
    "    sample_size = parsed[0]\n",
    "    preds = parsed[1:]\n",
    "    return sample_size, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a6d0774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_learner(results):\n",
    "    ensemble = []\n",
    "    for _, preds in results.values():\n",
    "        ensemble.append(preds)\n",
    "    ensemble = np.array(ensemble)\n",
    "    ensemble_mean = np.mean(ensemble, axis = 0)\n",
    "    return ensemble_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc610da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read in the arguments provided by the master server\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-cl','--client')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    client = args.client\n",
    "    path = '/gpfs/commons/groups/gursoy_lab/aelhussein/DCI_FL/'\n",
    "    # Import the data\n",
    "    X_train_norm, X_test_norm, X_val_norm, y_train, y_test, y_val = dataloader(f'{path}{client}/Data/')\n",
    "    \n",
    "    results = test_probability_clients(path, client)\n",
    "    ensemble_mean = ensemble_learner(results)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test.values, ensemble_mean)\n",
    "    ensemble_auc = metrics.auc(fpr, tpr)\n",
    "    print(f'{y_test.shape[0]}, {ensemble_auc}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
