{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77416f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 15:31:53.562204: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/slurm-20.11.0/lib64:\n",
      "2022-06-07 15:31:53.562258: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import contextlib\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import spur\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36209fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the data and create train/test split\n",
    "def dataloader(path):\n",
    "    ##load features and cohort data\n",
    "    X = pd.read_csv(path+'X.csv', index_col = 'hadm_id')\n",
    "    y = pd.read_csv(path+'y.csv', index_col = 'hadm_id')\n",
    "\n",
    "    ## train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.8, random_state=1)\n",
    "    #create validation set too\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "    ## create scaler and apply only to numeric data before adding binary data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_norm = scaler.fit_transform(X_train.iloc[:,:-5])\n",
    "    X_train_norm = pd.DataFrame(X_train_norm, index = X_train.index, columns = X_train.columns[:-5])\n",
    "    X_train_norm = X_train_norm.merge(X_train.iloc[:,-5:], left_index = True, right_index = True)\n",
    "\n",
    "    ##apply scaler to test data\n",
    "    X_test_norm = scaler.transform(X_test.iloc[:,:-5])\n",
    "    X_test_norm = pd.DataFrame(X_test_norm, index = X_test.index, columns = X_test.columns[:-5])\n",
    "    X_test_norm = X_test_norm.merge(X_test.iloc[:,-5:], left_index = True, right_index = True)\n",
    "    \n",
    "    ##apply scaler to val data\n",
    "    X_val_norm = scaler.transform(X_val.iloc[:,:-5])\n",
    "    X_val_norm = pd.DataFrame(X_val_norm, index = X_val.index, columns = X_val.columns[:-5])\n",
    "    X_val_norm = X_val_norm.merge(X_val.iloc[:,-5:], left_index = True, right_index = True)\n",
    "    \n",
    "    return X_train_norm, X_test_norm, X_val_norm, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d68567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create the keras model (LR in this case)\n",
    "def create_keras_model():\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=0)\n",
    "    ##build LR model\n",
    "    number_of_classes = 1\n",
    "    number_of_features = X_train_norm.shape[1]\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes,activation = 'sigmoid',input_dim = number_of_features))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc610da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read in the arguments provided by the master server\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-cl','--client')\n",
    "    parser.add_argument('-ep','--epochs', default = '20')\n",
    "    parser.add_argument('-ts','--test')\n",
    "    parser.add_argument('-en','--ensemble')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    client = args.client\n",
    "    epochs = args.epochs\n",
    "    test = args.test\n",
    "    ensemble = args.ensemble\n",
    "    \n",
    "    path = '/gpfs/commons/groups/gursoy_lab/aelhussein/DCI_FL/'\n",
    "    epochs = int(epochs)\n",
    "    # Import the data\n",
    "    X_train_norm, X_test_norm, X_val_norm, y_train, y_test, y_val = dataloader(f'{path}{client}/Data/')\n",
    "    # Load and run the neural network\n",
    "    client_model = tf.keras.models.load_model(f'{path}{client}/model/LR')\n",
    "    if test == 'False':\n",
    "        # Evaluate on the validation set    \n",
    "        validate_loss, validate_auc = client_model.evaluate(X_val_norm, y_val, verbose=0)\n",
    "        # Train model\n",
    "        history = tf.keras.callbacks.History()\n",
    "        with contextlib.redirect_stdout(None):\n",
    "            client_model.fit(X_train_norm, y_train, verbose=0, epochs=epochs, callbacks=[history])\n",
    "\n",
    "        # Save model\n",
    "        client_model.save(f'{path}{client}/model/LR')\n",
    "        # Calculate loss + auc\n",
    "        keys = list(history.history.keys())\n",
    "        train_loss = history.history[keys[0]][-1]\n",
    "        train_auc = history.history[keys[1]][-1]\n",
    "\n",
    "\n",
    "        # Send the weights back to master server\n",
    "        command = f'cp -r {path}{client}/model/LR/* {path}server/model/client_models/{client}/LR'\n",
    "        subprocess.call(command, shell = True) \n",
    "\n",
    "        # Reset stdout and print\n",
    "        print(len(X_train_norm), train_loss, train_auc, validate_loss, validate_auc)\n",
    "    \n",
    "    elif ensemble == 'True':\n",
    "        client_model = tf.keras.models.load_model(f'{path}{client}/model/')\n",
    "        predicted = client_model.predict(X_test_norm, verbose=0)\n",
    "        print(len(X_test_norm), predicted.reshape(1,-1))\n",
    "    \n",
    "    else:\n",
    "        client_model = tf.keras.models.load_model(f'{path}{client}/model/')\n",
    "        test_loss, test_auc = client_model.evaluate(X_test_norm, y_test, verbose=0)\n",
    "        print(len(X_test_norm), 0, 0, test_loss, test_auc)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
